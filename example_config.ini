# Neural Assembly Framework Configuration
# Example configuration for MNIST-like classification

[model]
# Network architecture
input_size = 8            # small input for quick tests
hidden_size = 4           # Hidden layer neurons
output_size = 3           # 3 classes for quick tests
num_layers = 2            # Number of hidden layers
activation = relu         # relu, sigmoid, tanh, softmax
dropout_rate = 0.5        # Dropout probability during training

[training]
epochs = 20               # Number of training epochs
batch_size = 4            # Mini-batch size
learning_rate = 0.01      # Initial learning rate
weight_decay = 0.0001     # L2 regularization
early_stopping = true     # Enable early stopping
patience = 10             # Epochs to wait before early stopping
lr_step_size = 5          # StepLR: decay lr every N epochs (0 to disable)
lr_gamma = 0.5            # StepLR: multiply lr by this factor

[optimizer]
type = adam               # sgd or adam
momentum = 0.9            # SGD momentum
beta1 = 0.9               # Adam beta1
beta2 = 0.999             # Adam beta2
epsilon = 1e-8            # Adam epsilon for numerical stability

[data]
train_file = csv/train.csv   # Training data file
test_file = csv/test.csv     # Test data file
train_label_file = csv/train_labels.csv
test_label_file = csv/test_labels.csv
val_split = 0.1               # Validation split ratio
shuffle = true                # Shuffle training data
normalize = true              # Normalize input features
