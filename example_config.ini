# Neural Assembly Framework Configuration
# Example configuration for MNIST-like classification

[model]
# Network architecture
input_size = 784          # 28x28 images flattened
hidden_size = 128         # Hidden layer neurons
output_size = 10          # 10 digit classes
num_layers = 2            # Number of hidden layers
activation = relu         # relu, sigmoid, tanh, softmax
dropout_rate = 0.5        # Dropout probability during training

[training]
epochs = 100              # Number of training epochs
batch_size = 32           # Mini-batch size
learning_rate = 0.001     # Initial learning rate
weight_decay = 0.0001     # L2 regularization
early_stopping = true     # Enable early stopping
patience = 10             # Epochs to wait before early stopping

[optimizer]
type = adam               # sgd or adam
momentum = 0.9            # SGD momentum
beta1 = 0.9               # Adam beta1
beta2 = 0.999             # Adam beta2
epsilon = 1e-8            # Adam epsilon for numerical stability

[data]
train_file = data/train.csv   # Training data file
test_file = data/test.csv     # Test data file
val_split = 0.1               # Validation split ratio
shuffle = true                # Shuffle training data
normalize = true              # Normalize input features
