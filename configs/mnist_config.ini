; =============================================================================
; MNIST Digit Classification Configuration
; =============================================================================
; Full MNIST: 28x28 = 784 inputs, 10 output classes (digits 0-9)
; Requires preprocessed CSV files with normalized pixel values
; =============================================================================

[model]
; 784 inputs -> 256 hidden -> 128 hidden -> 10 outputs
architecture = 784,256,128,10
hidden_activation = relu
output_activation = softmax

[training]
epochs = 20
; Larger batches for efficiency on big dataset
batch_size = 64
learning_rate = 0.001
; Adam works well for MNIST
optimizer = adam
; Adam hyperparameters
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8

[data]
; MNIST data files (preprocessed to CSV format)
; Each row: 784 normalized pixel values (0-1)
train_data = csv/mnist_train.csv
train_labels = csv/mnist_train_labels.csv
val_data = csv/mnist_test.csv  
val_labels = csv/mnist_test_labels.csv

[loss]
; Cross-entropy for multi-class classification
loss_function = cross_entropy

[logging]
print_every = 100
save_model = mnist_model.bin
save_checkpoint_every = 1
validate_every = 1

[options]
shuffle = true
random_seed = 12345
; Enable gradient clipping for stability
gradient_clip = 5.0
; Early stopping
early_stopping = true
patience = 3

[performance]
; Enable AVX-512 if available
use_simd = true
; Use multiple threads for large batches
num_threads = 4
