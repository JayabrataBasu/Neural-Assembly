; =============================================================================
; Deep Network Configuration
; =============================================================================
; Deeper architecture for complex problems
; Tests gradient flow through many layers
; =============================================================================

[model]
; 10 inputs -> 64 -> 64 -> 64 -> 64 -> 10 outputs
architecture = 10,64,64,64,64,10
hidden_activation = relu
output_activation = softmax

[training]
epochs = 100
batch_size = 128
learning_rate = 0.0001
; Adam essential for deep networks
optimizer = adam
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8

[data]
train_data = csv/deep_train.csv
train_labels = csv/deep_labels.csv
val_data = csv/deep_val.csv
val_labels = csv/deep_val_labels.csv

[loss]
loss_function = cross_entropy

[logging]
print_every = 10
save_model = deep_model.bin
validate_every = 5

[options]
shuffle = true
random_seed = 99999

[initialization]
; He initialization for ReLU networks
weight_init = he
bias_init = zero

[regularization]
weight_decay = 0.00001
; Batch normalization helps deep networks (if implemented)
batch_norm = false

[performance]
use_simd = true
num_threads = 8
