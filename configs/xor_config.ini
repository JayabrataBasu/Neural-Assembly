; =============================================================================
; XOR Example Configuration
; =============================================================================
; Classic XOR problem - demonstrates non-linear learning capability
; Network learns: 0 XOR 0 = 0, 0 XOR 1 = 1, 1 XOR 0 = 1, 1 XOR 1 = 0
; =============================================================================

[model]
; 2 inputs -> 8 hidden (with ReLU) -> 1 output (Sigmoid)
architecture = 2,8,1
; Activation: relu for hidden, sigmoid for output
hidden_activation = relu
output_activation = sigmoid

[training]
; Small dataset, many epochs needed
epochs = 10
batch_size = 4
learning_rate = 0.1
; Use SGD for this simple problem
optimizer = sgd
; Learning rate decay helps convergence
lr_decay = 0.999
lr_decay_steps = 100

[data]
; XOR training data (4 samples)
train_data = csv/xor_train.csv
train_labels = csv/xor_labels.csv
; Use same for validation
val_data = csv/xor_train.csv
val_labels = csv/xor_labels.csv

[loss]
; Binary cross-entropy for classification
loss_function = bce

[logging]
; Print often to see learning progress
print_every = 100
; Save model after training
save_model = xor_model.bin
; Validate to check overfitting
validate_every = 500

[options]
; Enable shuffling (though with 4 samples it matters less)
shuffle = true
; Seed for reproducibility
random_seed = 42
